{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers scikit-learn datasets faiss-cpu"
      ],
      "metadata": {
        "id": "fD-UjRqDdoX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxZWiTD0dm_g"
      },
      "outputs": [],
      "source": [
        "import time, numpy as np, torch\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, accuracy_score, normalized_mutual_info_score, adjusted_rand_score\n",
        "from sklearn.cluster import KMeans\n",
        "from datasets import load_dataset\n",
        "import faiss\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Base Interface"
      ],
      "metadata": {
        "id": "UECJIc1b_P0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingModel:\n",
        "    def encode(self, texts):\n",
        "        raise NotImplementedError\n",
        "\n",
        "# MiniLM baseline\n",
        "class MiniLMModel(EmbeddingModel):\n",
        "    def __init__(self):\n",
        "        self.model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    def encode(self, texts):\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "        return np.array(self.model.encode(texts, convert_to_numpy=True), dtype=\"float32\")\n",
        "\n",
        "# EmbeddingGemma\n",
        "class EmbeddingGemmaModel(EmbeddingModel):\n",
        "    def __init__(self, token=None):\n",
        "        self.model = SentenceTransformer(\"google/embeddinggemma-300m\", token=token)\n",
        "    def encode(self, texts):\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "        return np.array(self.model.encode(texts, convert_to_numpy=True), dtype=\"float32\")\n",
        "\n",
        "# Jina Code Embeddings\n",
        "class JinaCodeEmbeddingsModel(EmbeddingModel):\n",
        "    def __init__(self, token=None):\n",
        "        self.model = SentenceTransformer(\n",
        "            \"jinaai/jina-code-embeddings-1.5b\",\n",
        "            trust_remote_code=True, token=token\n",
        "        )\n",
        "    def encode(self, texts):\n",
        "        if isinstance(texts, str):\n",
        "            texts = [texts]\n",
        "        return np.array(self.model.encode(texts, convert_to_numpy=True), dtype=\"float32\")\n"
      ],
      "metadata": {
        "id": "o9MCoUXsd2Tl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Functions"
      ],
      "metadata": {
        "id": "ub45zXrl_ZAU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Similarity (STS)\n",
        "def eval_similarity(model):\n",
        "    dataset = load_dataset(\"mteb/stsbenchmark-sts\", split=\"test[:200]\")\n",
        "    sims, labels = [], []\n",
        "    for row in dataset:\n",
        "        e1 = model.encode(row[\"sentence1\"])[0]\n",
        "        e2 = model.encode(row[\"sentence2\"])[0]\n",
        "        sims.append(np.dot(e1, e2) / (np.linalg.norm(e1)*np.linalg.norm(e2)))\n",
        "        labels.append(row[\"score\"]/5.0)\n",
        "    return np.corrcoef(sims, labels)[0,1]\n",
        "\n",
        "# Classification\n",
        "def eval_classification(model, texts=None, labels=None):\n",
        "    if texts is None:\n",
        "        dataset = load_dataset(\"ag_news\", split=\"train[:1000]\")\n",
        "        texts = dataset[\"text\"]\n",
        "        labels = dataset[\"label\"]\n",
        "    X = model.encode(texts)\n",
        "    clf = LogisticRegression(max_iter=200).fit(X, labels)\n",
        "    preds = clf.predict(X)\n",
        "    return accuracy_score(labels, preds), f1_score(labels, preds, average=\"macro\")\n",
        "\n",
        "# Clustering\n",
        "def eval_clustering(model, texts=None, labels=None):\n",
        "    if texts is None:\n",
        "        dataset = load_dataset(\"ag_news\", split=\"test[:500]\")\n",
        "        texts = dataset[\"text\"]\n",
        "        labels = np.array(dataset[\"label\"])\n",
        "    else:\n",
        "        labels = np.array(labels)\n",
        "    X = model.encode(texts)\n",
        "    kmeans = KMeans(n_clusters=len(np.unique(labels)), n_init=10).fit(X)\n",
        "    return normalized_mutual_info_score(labels, kmeans.labels_), adjusted_rand_score(labels, kmeans.labels_)\n",
        "\n",
        "# Retrieval\n",
        "def eval_retrieval(model):\n",
        "    dataset = load_dataset(\"bansalaman18/msmarco-reranking-1m\", split=\"train[:1000]\")\n",
        "    queries = [row[\"query\"] for row in dataset]\n",
        "    documents = [row[\"document\"] for row in dataset]\n",
        "    labels = np.array([1 if row[\"relevant\"] == \"True\" else 0 for row in dataset])\n",
        "\n",
        "    query_embs = model.encode(queries)\n",
        "    doc_embs = model.encode(documents)\n",
        "\n",
        "    # Cosine similarity\n",
        "    sim_matrix = query_embs @ doc_embs.T\n",
        "    sim_matrix /= np.linalg.norm(query_embs, axis=1)[:, None]\n",
        "    sim_matrix /= np.linalg.norm(doc_embs, axis=1)[None, :]\n",
        "\n",
        "    top1 = np.argmax(sim_matrix, axis=1)\n",
        "    recall_at_1 = np.mean(labels[top1])\n",
        "    return recall_at_1\n",
        "\n",
        "# Efficiency\n",
        "def eval_efficiency(model):\n",
        "    texts = [\"The quick brown fox jumps over the lazy dog.\"] * 256\n",
        "    start = time.time()\n",
        "    emb = model.encode(texts)\n",
        "    dur = time.time() - start\n",
        "    return emb.shape[1], len(texts)/dur\n",
        "\n"
      ],
      "metadata": {
        "id": "lJ_GAvaOd5tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load MeTTa JSON Dataset"
      ],
      "metadata": {
        "id": "87JMn6YyI0wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_metta_json(path=\"metta_dataset.json\"):\n",
        "    with open(path, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "    texts = [f\"{item['code']} # {item['comment']}\" for item in data]\n",
        "    labels = [item['label'] for item in data]\n",
        "    return texts, labels"
      ],
      "metadata": {
        "id": "GhC1WCRwI0A7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Benchmarks"
      ],
      "metadata": {
        "id": "Y3WdqC-Z_oH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token = userdata.get(\"embeddingGemma\")\n",
        "MODELS = {\n",
        "    \"MiniLM\": MiniLMModel(),\n",
        "    \"EmbeddingGemma\": EmbeddingGemmaModel(token=token),\n",
        "    \"JinaCode\": JinaCodeEmbeddingsModel(token=token)\n",
        "}"
      ],
      "metadata": {
        "id": "lB2PS5A7eIDn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b64a3828-f30a-43d2-ff0b-06c9366a89be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for name, model in MODELS.items():\n",
        "    print(f\"\\nðŸ”¹ Evaluating {name}...\")\n",
        "\n",
        "    sim = eval_similarity(model)\n",
        "    acc, f1 = eval_classification(model)\n",
        "    nmi, ari = eval_clustering(model)\n",
        "    recall = eval_retrieval(model)\n",
        "    dim, speed = eval_efficiency(model)\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"Similarity\": round(sim,3),\n",
        "        \"Cls Acc\": round(acc,3),\n",
        "        \"Cls F1\": round(f1,3),\n",
        "        \"Cluster NMI\": round(nmi,3),\n",
        "        \"Cluster ARI\": round(ari,3),\n",
        "        \"Retrieval R@10\": round(recall,3),\n",
        "        \"Dim\": dim,\n",
        "        \"Throughput (txt/s)\": round(speed,2)\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "print(\"\\n===== Benchmark Results =====\\n\")\n",
        "print(df)"
      ],
      "metadata": {
        "id": "csX4Ip9JeK1q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79c6d4fa-5981-4b1f-f3e0-a05765e2ad4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ”¹ Evaluating MiniLM...\n",
            "\n",
            "ðŸ”¹ Evaluating EmbeddingGemma...\n",
            "\n",
            "ðŸ”¹ Evaluating JinaCode...\n",
            "\n",
            "===== Benchmark Results =====\n",
            "\n",
            "            Model  Similarity  Cls Acc  Cls F1  Cluster NMI  Cluster ARI  \\\n",
            "0          MiniLM       0.923    0.899   0.886        0.415        0.312   \n",
            "1  EmbeddingGemma       0.650    0.875   0.859        0.520        0.538   \n",
            "2        JinaCode       0.820    0.896   0.882        0.568        0.588   \n",
            "\n",
            "   Retrieval R@10   Dim  Throughput (txt/s)  \n",
            "0           0.475   384             2650.60  \n",
            "1           0.025   768              266.40  \n",
            "2           0.275  1536              117.36  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YfdZAMl2DExa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}